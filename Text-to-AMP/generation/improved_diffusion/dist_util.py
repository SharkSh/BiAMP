import io
import os
import torch as th
import torch.distributed as dist

""" è®¾ç½® PyTorch åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼ˆä¸ä¾èµ– MPIï¼‰ã€‚"""
def setup_dist():
    if dist.is_initialized():
        return
    print("PyTorch version:", th.__version__)
    print("CUDA available:", th.cuda.is_available())
    print("CUDA version:", th.version.cuda)
    print("CUDA device count:", th.cuda.device_count())

    if th.cuda.is_available():
        print("CUDA device name:", th.cuda.get_device_name(0))
    else:
        print("No GPU detected. Check your environment and hardware.")

    backend = "nccl" if th.cuda.is_available() else "gloo"

    # ä»ç¯å¢ƒå˜é‡è·å– `RANK` å’Œ `WORLD_SIZE`
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])

    # è®¾å®šä¸»è¿›ç¨‹çš„åœ°å€å’Œç«¯å£
    os.environ.setdefault("MASTER_ADDR", "127.0.0.1")
    os.environ.setdefault("MASTER_PORT", "29500") 

    # åˆå§‹åŒ– PyTorch åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)

""" è·å–å½“å‰ GPU è®¾å¤‡ã€‚"""
def dev():
    if th.cuda.is_available():
        local_rank = int(os.environ["LOCAL_RANK"])  # `torchrun` ä¼ é€’çš„ LOCAL_RANK
        return th.device(f"cuda:{local_rank}")
    return th.device("cpu")

""" Load a PyTorch file without redundant fetches across ranks. Now using torch.distributed instead of MPI. """
def load_state_dict(path, **kwargs):
    rank = dist.get_rank()
    obj_list = [None]

    if rank == 0:
        with open(path, "rb") as f:
            obj_list[0] = f.read()

    # Broadcast the state_dict from rank 0 to all other ranks
    dist.broadcast_object_list(obj_list, src=0)

    return th.load(io.BytesIO(obj_list[0]), **kwargs)

""" ç¡®ä¿æ¯å¼ å¡ä¸Šæ¨¡å‹å‚æ•°ä¸€è‡´. """
def sync_params(params):
    for p in params:
        with th.no_grad():
            dist.broadcast(p, src=0)

""" æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒæœ‰æ²¡æœ‰æ­£å¸¸å¯åŠ¨ """
def check():
    rank = int(os.environ["RANK"])
    # **æ£€æŸ¥ 1: è¿›ç¨‹æ˜¯å¦æ­£ç¡®å¯åŠ¨**
    if dist.is_initialized():
        print(f"âœ… Distributed initialized: Rank {dist.get_rank()} / {dist.get_world_size()}")
    else:
        print("âŒ Distributed initialization failed!")

    # **æ£€æŸ¥ 2: æ¯ä¸ª Rank æ‰“å°è®¾å¤‡ä¿¡æ¯**
    device = dev()
    print(f"ğŸ–¥ï¸ Rank {rank} initialized with device: {device}")

    # **æ£€æŸ¥ 3: æ‰€æœ‰ rank è¿›è¡Œ barrier åŒæ­¥**
    dist.barrier()
    print(f"âœ… Rank {dist.get_rank()} passed the barrier!")

    # **æ£€æŸ¥ 4: æµ‹è¯• tensor å¹¿æ’­**
    tensor = th.tensor([0.0], device=device)
    if rank == 0:
        tensor += 10.0
    dist.broadcast(tensor, src=0)
    print(f"ğŸ“¡ Rank {dist.get_rank()} received tensor: {tensor.item()}")

